# src/attachment_processor.py
"""é™„ä»¶å¤„ç†æ¨¡å— - ä¸“é—¨å¤„ç†ç®€å†é™„ä»¶è§£æ"""

import os
import io
import json
import logging
import re
from typing import List, Dict, Optional
import base64

try:
    import docx

    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    logging.warning("python-docx not installed. Word document processing disabled.")

try:
    import PyPDF2

    PDF_AVAILABLE = True
except ImportError:
    try:
        import pdfplumber

        PDF_AVAILABLE = True
        PDF_LIBRARY = "pdfplumber"
    except ImportError:
        PDF_AVAILABLE = False
        logging.warning(
            "Neither PyPDF2 nor pdfplumber installed. PDF processing disabled."
        )
    else:
        PDF_LIBRARY = "pdfplumber"
else:
    PDF_LIBRARY = "PyPDF2"

try:
    import openpyxl

    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
    logging.warning("openpyxl not installed. Excel document processing disabled.")

import httpx
from openai import AsyncOpenAI
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


class ResumeData(BaseModel):
    """ç®€å†æ•°æ®æ¨¡å‹"""

    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    gender: Optional[str] = None
    age: Optional[str] = None
    nationality: Optional[str] = None
    nearest_station: Optional[str] = None
    education: Optional[str] = None
    arrival_year_japan: Optional[str] = None
    certifications: List[str] = Field(default_factory=list)
    skills: List[str] = Field(default_factory=list)
    technical_keywords: List[str] = Field(default_factory=list)
    experience: str = ""
    work_scope: Optional[str] = None
    work_experience: Optional[str] = None
    japanese_level: Optional[str] = None
    english_level: Optional[str] = None
    availability: Optional[str] = None
    preferred_work_style: List[str] = Field(default_factory=list)
    preferred_locations: List[str] = Field(default_factory=list)
    desired_rate_min: Optional[int] = None
    desired_rate_max: Optional[int] = None
    overtime_available: Optional[bool] = False
    business_trip_available: Optional[bool] = False
    self_promotion: Optional[str] = None
    remarks: Optional[str] = None
    recommendation: Optional[str] = None
    source_filename: Optional[str] = None


class AttachmentProcessor:
    """é™„ä»¶å¤„ç†å™¨"""

    def __init__(self, ai_config: Dict):
        self.ai_config = ai_config
        self.ai_client = None

        provider_name = ai_config.get("provider_name")
        api_key = ai_config.get("api_key")

        if provider_name == "openai":
            if api_key:
                self.ai_client = AsyncOpenAI(api_key=api_key)
        elif provider_name == "deepseek":
            api_base_url = ai_config.get("api_base_url")
            timeout = ai_config.get("timeout", 120.0)
            if api_key and api_base_url:
                self.ai_client = httpx.AsyncClient(
                    base_url=api_base_url,
                    headers={
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json",
                    },
                    timeout=timeout,
                )
                logger.info("AttachmentProcessor AI client initialized")

    def extract_text_from_docx(self, file_content: bytes) -> str:
        """ä»Wordæ–‡æ¡£æå–æ–‡æœ¬"""
        if not DOCX_AVAILABLE:
            logger.error("python-docx not available for Word document processing")
            return ""

        try:
            doc = docx.Document(io.BytesIO(file_content))
            full_text = []

            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())

            # å¤„ç†è¡¨æ ¼ä¸­çš„æ–‡æœ¬
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            full_text.append(cell.text.strip())

            text = "\n".join(full_text)
            logger.info(f"ä»Wordæ–‡æ¡£æå–äº† {len(text)} å­—ç¬¦çš„æ–‡æœ¬")
            return text

        except Exception as e:
            logger.error(f"Wordæ–‡æ¡£è§£æå¤±è´¥: {e}")
            return ""

    def extract_text_from_pdf(self, file_content: bytes) -> str:
        """ä»PDFæ–‡ä»¶æå–æ–‡æœ¬"""
        if not PDF_AVAILABLE:
            logger.error("PDF processing library not available")
            return ""

        try:
            if PDF_LIBRARY == "pdfplumber":
                import pdfplumber

                full_text = []
                with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                    for page in pdf.pages:
                        text = page.extract_text()
                        if text:
                            full_text.append(text)

                text = "\n".join(full_text)
                logger.info(f"ä»PDFæ–‡æ¡£æå–äº† {len(text)} å­—ç¬¦çš„æ–‡æœ¬")
                return text

            else:  # PyPDF2
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
                full_text = []

                for page in pdf_reader.pages:
                    text = page.extract_text()
                    if text:
                        full_text.append(text)

                text = "\n".join(full_text)
                logger.info(f"ä»PDFæ–‡æ¡£æå–äº† {len(text)} å­—ç¬¦çš„æ–‡æœ¬")
                return text

        except Exception as e:
            logger.error(f"PDFæ–‡æ¡£è§£æå¤±è´¥: {e}")
            return ""

    def extract_text_from_excel(self, file_content: bytes) -> str:
        """ä»Excelæ–‡ä»¶æå–æ–‡æœ¬"""
        if not EXCEL_AVAILABLE:
            logger.error("openpyxl not available for Excel document processing")
            return ""

        try:
            workbook = openpyxl.load_workbook(io.BytesIO(file_content))
            full_text = []

            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                sheet_text = []

                for row in sheet.iter_rows(values_only=True):
                    row_text = []
                    for cell in row:
                        if cell is not None and str(cell).strip():
                            row_text.append(str(cell).strip())
                    if row_text:
                        sheet_text.append(" | ".join(row_text))

                if sheet_text:
                    full_text.append(f"=== {sheet_name} ===")
                    full_text.extend(sheet_text)

            text = "\n".join(full_text)
            print(text)  # ğŸ‘ˆ æ§åˆ¶å°è¾“å‡ºå†…å®¹
            logger.info(f"ä»Excelæ–‡æ¡£æå–äº† {len(text)} å­—ç¬¦çš„æ–‡æœ¬")
            return text

        except Exception as e:
            logger.error(f"Excelæ–‡æ¡£è§£æå¤±è´¥: {e}")
            return ""

    def extract_text_from_attachment(self, attachment: Dict) -> str:
        """æ ¹æ®é™„ä»¶ç±»å‹æå–æ–‡æœ¬å†…å®¹"""
        filename = attachment.get("filename", "").lower()
        file_content = attachment.get("content", b"")

        if not file_content:
            logger.warning(f"é™„ä»¶ {filename} æ²¡æœ‰å†…å®¹")
            return ""

        # å¦‚æœå†…å®¹æ˜¯base64ç¼–ç çš„å­—ç¬¦ä¸²ï¼Œå…ˆè§£ç 
        if isinstance(file_content, str):
            try:
                file_content = base64.b64decode(file_content)
            except Exception as e:
                logger.error(f"Base64è§£ç å¤±è´¥: {e}")
                return ""

        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è§£ææ–¹æ³•
        if filename.endswith((".docx", ".doc")):
            return self.extract_text_from_docx(file_content)
        elif filename.endswith(".pdf"):
            return self.extract_text_from_pdf(file_content)
        elif filename.endswith((".xlsx", ".xls")):
            return self.extract_text_from_excel(file_content)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")
            return ""

    def _extract_json_from_text(self, text: str) -> Optional[Dict]:
        """ä»AIå“åº”æ–‡æœ¬ä¸­æå–JSON"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            result = json.loads(text.strip())
            return result
        except json.JSONDecodeError:
            # å°è¯•æŸ¥æ‰¾JSONå—
            json_pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
            matches = re.findall(json_pattern, text, re.DOTALL)

            for match in matches:
                try:
                    result = json.loads(match)
                    return result
                except json.JSONDecodeError:
                    continue

            # å°è¯•å¤æ‚çš„JSONæå–
            start_idx = text.find("{")
            if start_idx != -1:
                brace_count = 0
                for i, char in enumerate(text[start_idx:], start_idx):
                    if char == "{":
                        brace_count += 1
                    elif char == "}":
                        brace_count -= 1
                        if brace_count == 0:
                            try:
                                extracted = text[start_idx : i + 1]
                                result = json.loads(extracted)
                                return result
                            except json.JSONDecodeError:
                                break

            logger.warning(f"æ— æ³•ä»æ–‡æœ¬ä¸­æå–JSON: {text[:200]}...")
            return None

    async def extract_resume_data_with_ai(
        self, resume_text: str, filename: str = ""
    ) -> Optional[ResumeData]:
        """ä½¿ç”¨AIä»ç®€å†æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ•°æ®"""
        logger.warning(f"resume_text: {resume_text}...")
        if not self.ai_client:
            logger.warning(
                "AI client not initialized. Skipping resume data extraction."
            )
            return None

        if not resume_text.strip():
            logger.warning("ç®€å†æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡æå–")
            return None

        provider_name = self.ai_config.get("provider_name")
        model_extract = self.ai_config.get("model_extract", "gpt-4")
        temperature = self.ai_config.get("temperature", 0.3)
        max_tokens_extract = self.ai_config.get("max_tokens", 2048)

        # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¶…å‡ºAIæ¨¡å‹é™åˆ¶
        if len(resume_text) > 4000:
            resume_text = resume_text[:4000] + "..."

        prompt = f"""
ä»¥ä¸‹ã¯å±¥æ­´æ›¸ãƒ»è·å‹™çµŒæ­´æ›¸ã®å†…å®¹ã§ã™ã€‚ã“ã®æƒ…å ±ã‹ã‚‰æŠ€è¡“è€…ã®è©³ç´°æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã€å¿…ãšJSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ•ã‚¡ã‚¤ãƒ«åã€‘: {filename}
ã€å±¥æ­´æ›¸å†…å®¹ã€‘:
{resume_text}

ä»¥ä¸‹ã®å½¢å¼ã§æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
{{
    "name": "æŠ€è¡“è€…åï¼ˆå¿…é ˆï¼‰",
    "email": "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹",
    "phone": "é›»è©±ç•ªå·",
    "gender": "æ€§åˆ¥ï¼ˆç”·æ€§/å¥³æ€§/å›ç­”ã—ãªã„ï¼‰",
    "age": "å¹´é½¢",
    "nationality": "å›½ç±",
    "nearest_station": "æœ€å¯„ã‚Šé§…",
    "education": "å­¦æ­´ãƒ»æœ€çµ‚å­¦æ­´",
    "arrival_year_japan": "æ¥æ—¥å¹´åº¦",
    "certifications": ["è³‡æ ¼1", "è³‡æ ¼2"],
    "skills": ["ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª1", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª2", "æŠ€è¡“ã‚¹ã‚­ãƒ«1"],
    "technical_keywords": ["Java", "Python", "AWS", "React"],
    "experience": "ç·çµŒé¨“å¹´æ•°ï¼ˆä¾‹ï¼š5å¹´ï¼‰",
    "work_scope": "ä½œæ¥­ç¯„å›²ãƒ»å¾—æ„åˆ†é‡",
    "work_experience": "è·å‹™çµŒæ­´ã®è©³ç´°",
    "japanese_level": "æ—¥æœ¬èªãƒ¬ãƒ™ãƒ«ï¼ˆä¸å•/æ—¥å¸¸ä¼šè©±ãƒ¬ãƒ™ãƒ«/ãƒ“ã‚¸ãƒã‚¹ãƒ¬ãƒ™ãƒ«/ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ¬ãƒ™ãƒ«ï¼‰",
    "english_level": "è‹±èªãƒ¬ãƒ™ãƒ«ï¼ˆä¸å•/æ—¥å¸¸ä¼šè©±ãƒ¬ãƒ™ãƒ«/ãƒ“ã‚¸ãƒã‚¹ãƒ¬ãƒ™ãƒ«/ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ¬ãƒ™ãƒ«ï¼‰",
    "availability": "ç¨¼åƒå¯èƒ½æ™‚æœŸ",
    "preferred_work_style": ["å¸¸é§", "ãƒªãƒ¢ãƒ¼ãƒˆ", "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰"],
    "preferred_locations": ["æ±äº¬", "å¤§é˜ª"],
    "desired_rate_min": å¸Œæœ›å˜ä¾¡ä¸‹é™ï¼ˆæ•°å€¤ã®ã¿ã€ä¸‡å††å˜ä½ï¼‰,
    "desired_rate_max": å¸Œæœ›å˜ä¾¡ä¸Šé™ï¼ˆæ•°å€¤ã®ã¿ã€ä¸‡å††å˜ä½ï¼‰,
    "overtime_available": æ®‹æ¥­å¯¾å¿œå¯èƒ½ï¼ˆtrue/falseï¼‰,
    "business_trip_available": å‡ºå¼µå¯¾å¿œå¯èƒ½ï¼ˆtrue/falseï¼‰,
    "self_promotion": "è‡ªå·±PRãƒ»ã‚¢ãƒ”ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒˆ",
    "remarks": "å‚™è€ƒãƒ»ãã®ä»–",
    "recommendation": "æ¨è–¦ã‚³ãƒ¡ãƒ³ãƒˆ",
    "source_filename": "{filename}"
}}

é‡è¦ãªæŒ‡ç¤ºï¼š
1. nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¿…é ˆã§ã™ã€‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯"åå‰ä¸æ˜"ã¨ã—ã¦ãã ã•ã„
2. æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„é …ç›®ã¯nullã«ã—ã¦ãã ã•ã„
3. desired_rate_min/maxã¯æ•°å€¤ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆä¸‡å††è¡¨è¨˜ã¯é™¤ãï¼‰
4. skillsã«ã¯å…·ä½“çš„ãªæŠ€è¡“åã‚’å«ã‚ã¦ãã ã•ã„
5. technical_keywordsã«ã¯æŠ€è¡“é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
6. JSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€ä»–ã®èª¬æ˜ã¯ä¸è¦ã§ã™
"""

        messages = [
            {
                "role": "system",
                "content": "ã‚ãªãŸã¯å±¥æ­´æ›¸ãƒ»è·å‹™çµŒæ­´æ›¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚å¿…ãšJSONã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚",
            },
            {"role": "user", "content": prompt},
        ]

        try:
            if provider_name == "openai":
                response = await self.ai_client.chat.completions.create(
                    model=model_extract,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens_extract,
                )
                raw_content = response.choices[0].message.content
                data = self._extract_json_from_text(raw_content)

            elif provider_name == "deepseek":
                if isinstance(self.ai_client, httpx.AsyncClient):
                    try:
                        logger.info(f"å‘é€ç®€å†è§£æè¯·æ±‚åˆ°DeepSeek API: {filename}")

                        response = await self.ai_client.post(
                            "/v1/chat/completions",
                            json={
                                "model": model_extract,
                                "messages": messages,
                                "temperature": temperature,
                                "max_tokens": max_tokens_extract,
                            },
                        )
                        response.raise_for_status()

                        response_json = response.json()
                        raw_response_content = response_json["choices"][0]["message"][
                            "content"
                        ]

                        logger.info(f"=== DeepSeek ç®€å†è§£æå“åº” ({filename}) ===")
                        logger.info(f"Raw content:\n{raw_response_content}")

                        data = self._extract_json_from_text(raw_response_content)
                        if data:
                            logger.info(f"æˆåŠŸè§£æç®€å†JSON: {filename}")
                        else:
                            logger.error(f"JSONè§£æå¤±è´¥: {filename}")

                    except Exception as e:
                        logger.error(f"DeepSeek API error for resume {filename}: {e}")
                        return None
                else:
                    logger.warning(
                        "DeepSeek client not available for resume extraction"
                    )
                    return None
            else:
                logger.warning(
                    f"Unsupported AI provider for resume extraction: {provider_name}"
                )
                return None

            if data:
                # ç¡®ä¿nameå­—æ®µå­˜åœ¨
                if not data.get("name"):
                    data["name"] = "åå‰ä¸æ˜"

                # ç¡®ä¿experienceå­—æ®µå­˜åœ¨
                if not data.get("experience"):
                    data["experience"] = "ä¸æ˜"

                return ResumeData(**data)
            else:
                logger.error(
                    f"Failed to parse JSON from AI response for resume: {filename}"
                )
                return None

        except Exception as e:
            logger.error(f"Error extracting resume data from {filename}: {e}")
            import traceback

            logger.error(f"Full traceback: {traceback.format_exc()}")
            return None

    async def process_resume_attachments(
        self, attachments: List[Dict]
    ) -> List[ResumeData]:
        """å¤„ç†æ‰€æœ‰ç®€å†é™„ä»¶ï¼Œè¿”å›æå–çš„ç®€å†æ•°æ®åˆ—è¡¨"""
        resume_data_list = []

        # è¿‡æ»¤å‡ºå¯èƒ½çš„ç®€å†æ–‡ä»¶
        resume_files = []
        resume_extensions = [".docx", ".doc", ".pdf", ".xlsx", ".xls"]
        engineer_patterns = [
            r"å±¥æ­´æ›¸",
            r"è·å‹™çµŒæ­´",
            r"ã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆ",
            r"resume",
            r"cv",
            r"profile",
        ]

        for attachment in attachments:
            filename = attachment.get("filename", "").lower()

            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            has_resume_extension = any(
                filename.endswith(ext) for ext in resume_extensions
            )

            # æ£€æŸ¥æ–‡ä»¶åå…³é”®è¯
            has_resume_keyword = any(
                re.search(pattern, filename) for pattern in engineer_patterns
            )

            if has_resume_extension or has_resume_keyword:
                resume_files.append(attachment)
                logger.info(f"å‘ç°å¯èƒ½çš„ç®€å†æ–‡ä»¶: {filename}")

        if not resume_files:
            logger.info("æœªå‘ç°ç®€å†é™„ä»¶")
            return resume_data_list

        logger.info(f"å¼€å§‹å¤„ç† {len(resume_files)} ä¸ªç®€å†æ–‡ä»¶")

        for attachment in resume_files:
            filename = attachment.get("filename", "")
            logger.info(f"æ­£åœ¨å¤„ç†ç®€å†æ–‡ä»¶: {filename}")

            try:
                # æå–æ–‡æœ¬å†…å®¹
                resume_text = self.extract_text_from_attachment(attachment)

                if not resume_text.strip():
                    logger.warning(f"æ— æ³•ä»æ–‡ä»¶ {filename} ä¸­æå–æ–‡æœ¬å†…å®¹")
                    continue

                logger.info(f"ä» {filename} æå–äº† {len(resume_text)} å­—ç¬¦çš„æ–‡æœ¬")

                # ä½¿ç”¨AIæå–ç»“æ„åŒ–æ•°æ®
                resume_data = await self.extract_resume_data_with_ai(
                    resume_text, filename
                )

                if resume_data:
                    resume_data_list.append(resume_data)
                    logger.info(f"æˆåŠŸæå–ç®€å†æ•°æ®: {resume_data.name} ({filename})")
                else:
                    logger.error(f"æ— æ³•ä» {filename} æå–ç®€å†æ•°æ®")

            except Exception as e:
                logger.error(f"å¤„ç†ç®€å†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                continue

        logger.info(f"ç®€å†å¤„ç†å®Œæˆï¼ŒæˆåŠŸæå– {len(resume_data_list)} ä»½ç®€å†æ•°æ®")
        return resume_data_list

    def has_resume_attachments(self, attachments: List[Dict]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç®€å†é™„ä»¶"""
        if not attachments:
            return False

        resume_extensions = [".docx", ".doc", ".pdf", ".xlsx", ".xls"]
        engineer_patterns = [
            r"å±¥æ­´æ›¸",
            r"è·å‹™çµŒæ­´",
            r"ã‚¹ã‚­ãƒ«ã‚·ãƒ¼ãƒˆ",
            r"resume",
            r"cv",
            r"profile",
        ]

        for attachment in attachments:
            filename = attachment.get("filename", "").lower()

            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            has_resume_extension = any(
                filename.endswith(ext) for ext in resume_extensions
            )

            # æ£€æŸ¥æ–‡ä»¶åå…³é”®è¯
            has_resume_keyword = any(
                re.search(pattern, filename) for pattern in engineer_patterns
            )

            if has_resume_extension or has_resume_keyword:
                return True

        return False
